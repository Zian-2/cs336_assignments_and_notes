# 6 Generating text

既然我们已经可以训练模型了，最后需要从模型中生成文本的能力。  

语言模型接收长度为 sequence_length 的整数序列（可以是批处理的），并产生一个大小为 (sequence_length × vocab_size) 的矩阵，其中序列的每个元素都是预测该位置之后下一个词的概率分布。我们现在将编写几个函数，将其转化为新序列的采样方案。 

**Softmax**  按照标准惯例，语言模型的输出是最后一个线性层的输出（即 logits），因此我们必须通过在公式 10 中见过的 softmax 操作将其转化为归一化的概率。  

**解码**  为了从模型中生成文本（解码），我们将为模型提供一个前缀标记序列（即 prompt），并要求它产生一个在词表上的概率分布，以预测序列中的下一个词。然后，我们将从这个词表项的分布中进行采样，以确定下一个输出标记。

具体来说，解码过程的一个步骤应该接收一个序列 $x_{1...t}$，并通过以下方程返回一个标记 $x_{t+1}$：

$$P(x_{t+1} = i | x_{1...t}) = \frac{\exp(v_i)}{\sum_j \exp(v_j)}$$

$$v = TransformerLM(x_{1...t})_t \in \mathbb{R}^{vocab\_size}$$

其中 TransformerLM 是我们的模型，它输入长度为 sequence_length 的序列并产生一个大小为 (sequence_length × vocab_size) 的矩阵，我们取该矩阵的最后一个元素，因为我们正在寻找第 t 个位置的下一个词预测。  

通过反复从这些单步条件分布中采样（将我们之前生成的输出标记附加到下一个解码时间步的输入中），直到生成序列结束标记 <|endoftext|>（或达到用户指定的生成标记最大数量），这就给了我们一个基本的解码器。

**解码技巧**  我们将尝试使用小型模型，而小型模型有时会生成质量非常低的文本。两个简单的解码技巧可以帮助解决这些问题。首先，在温度缩放(temperature scaling)中，我们使用温度参数 τ 修改 softmax，新的 softmax 为：

$$softmax(v, \tau)_i = \frac{\exp(v_i/\tau)}{\sum_{j=1}^{|vocab\_size|} \exp(v_j/\tau)}$$

注意设置 τ → 0 会使 v 中最大的元素占据主导地位（相当于贪婪搜索），softmax 的输出变成集中在这个最大元素上的 one-hot 向量。 

其次，另一个技巧是nucleus采样或 top-p 采样，我们通过截断低概率词来修改采样分布。设 q 为我们从大小为 (vocab_size) 的（经过温度缩放的）softmax 中得到的概率分布。使用超参数 p 的核采样根据以下方程生成下一个标记： 

$$P(x_{t+1} = i | q) = \begin{cases} \frac{q_i}{\sum_{j \in V(p)} q_j} & \text{如果 } i \in V(p) \\ 0 & \text{否则} \end{cases}$$

其中 V(p) 是满足 $\sum_{j \in V(p)} q_j \geq p$ 的最小索引集合。你可以通过首先按大小对概率分布 q 进行排序，并选择最大的词表元素直到达到目标水平 p，从而轻松计算这个数值。  

## Problem (decoding)  (3 分)

实现一个从你的语言模型进行解码的函数。我们建议你支持以下功能：  

为用户提供的提示词生成补全（即接收某些 x1...t 并进行采样补全，直到遇到 <|endoftext|> 标记）。  

允许用户控制生成的最大标记数量。  
 
给定所需的温度值，在采样前对预测的下一词分布应用 softmax 温度缩放。  
 
给定用户指定的阈值，进行 Top-p 采样。

### Answer：
见transformer_generation.py